#!/usr/bin/env python3
from pymongo import MongoClient
from multiprocessing import Process, Queue
from threading import Thread
import multiprocessing
import threading
from timeit import default_timer as timer
import timeit
import random
import datetime
import json
import math
import time
import _pickle as pickle
from prettytable import PrettyTable
import csv
import os
import global_config
import read_config
import sys

# CONSIDER: Using a Queue between the query generation and query execution 
#             Many processes just pull queries off a queue fo execution,
#             queries are randomly generated by a smaller number of generation processes
#             should increase concurrency. But would need to change result handling to use read groups
#             rather than procs to track performance

def main():
    # Retrieve basic read configuration from config file
    print("Retrieving read config....")
    readDurationSeconds = read_config.readDurationSeconds
    procsRatioList = read_config.procsRatio
    docsRatioList = read_config.docsRatio

    # Count docs in collection (informs docReadingGroup Ranges)
    print("Counting Existing Documents in Collection")
    totalDocs = getTotalDocs()
    
    # Create reading groups (function returns a list of reading group dicts)
    print("Creating Reading Groups")
    readingGroupsList = createReadingGroupsList(procsRatioList, docsRatioList, totalDocs)

    # Print reading groups (formatted as table)
    printReadingGroups(readingGroupsList)
    
    # Create a queue for each process to add individual request response metrics results to
    responseMetricsQueue = Queue()

    # Construct the list of processes
    processList = createProcessList(readingGroupsList, readDurationSeconds, responseMetricsQueue)
    
    # Start each process 
    startReadProcesses(processList)

    # Sleep 1s while procs start then initiate the results handler
    # Note: the 1s also helps ensure processes will exit before resultsHandler stops looping
    time.sleep(1)
    resultsHandler(responseMetricsQueue, readDurationSeconds, readingGroupsList)

    # Wait for read processes to complete/exit before exiting main (may not be necessary)
    joinReadProcesses(processList)

def getTotalDocs():
    # Define the mongoclient/collection
    coll = getMongoCollection()

    # Return the count of docs in collection
    return coll.count()

def getMongoCollection():
    # Define mongoclient based on global_config
    client = MongoClient(global_config.mongo_uri)
    db = client[global_config.dbname]
    coll = db[global_config.collname]

    return coll

def createReadingGroupsList(procsRatioList, docsRatioList, totalDocs):
    # procsRatioList and docsRatioList must have the same length
    # each pair of entries (one from each list) generates a docReadingGroup
    if (len(procsRatioList) != len(docsRatioList)):
        print()
        print("procRatioList (length: " + str(len(procsRatioList)) + ") and docsRatioList (length: " + str(len(docsRatioList)) + ") must be the same length")
        sys.exit("Error in read_config file")
    
    readingGroupsList = []

    # For each entry in the procsRatioList
    for i in range(len(procsRatioList)):
        procs = procsRatioList[i]
        
        # docsRatio = entry / sum of entries
        docsRatio = (docsRatioList[i] / sum(docsRatioList))
        
        if (len(readingGroupsList) == 0):
            # Set lower boundary of range to 0 if first entry in list
            lower = 0
        else:
            # Set lower boundry of range to upper boundary of previous entry in list
            lower = readingGroupsList[-1]["docsUpper"]
        
        # Set upper to the lower plus an increment based on docsRatio * totalDocs
        upper = lower + (math.floor(docsRatio * totalDocs))

        # Add reading group to list
        readingGroupsList.append({
            "readingGroupId": i,
            "procs": procs,
            "docsLower": lower,
            "docsUpper": upper,
            "docsInRange": (upper - lower)
            })

    # Return list of reading groups
    return readingGroupsList

def printReadingGroups(readingGroupsList):
    # Initialise the PrettyTable
    table = PrettyTable()
    
    # Set the table title
    table.title = "Reading Groups"

    # Initialise the fields list
    fields = []

    # Add each key in the first readingGroups list as fields to the table
    for key in readingGroupsList[0].keys():
        fields.append(key)
    table.field_names = fields
    
    # For each item in the readingGroups list add a row to the table
    for readingGroup in readingGroupsList:
        row = []

        # Add the values of each field to the row list
        for field in readingGroup:
            row.append(readingGroup[field])
        
        # add the row to the table
        table.add_row(row)

    print()
    print(table)
    print()

def createProcessList(readingGroupsList, readDurationSeconds, responseMetricsQueue):
    # Initialise process list
    processList = []
    
    # For each readingGroup in the list
    for readingGroup in readingGroupsList:
        # Add to the processList the number of processes defined in the 'procs' field
        for p in range(readingGroup["procs"]):
            process = Process(target=defineAndRunThreads,args=(readingGroup, readDurationSeconds, responseMetricsQueue))
            processList.append(process)

    # Return the list of processes
    return processList

def startReadProcesses(processList):
    try:
        # Start each process in the list
        for process in processList:
            process.start()
    except: 
        print("Error executing processes in processList")

def defineAndRunThreads(readingGroup, durationSeconds, responseMetricsQueue):
    # Initialise Thread list
    threadList = []
    threadsPerProc = read_config.threadsPerProc
    coll = getMongoCollection()
    
    # Add to the threadList the number of threads defined in the config file
    for t in range(threadsPerProc):
        thread = Thread(target=findRandDocsForDuration,args=(readingGroup, durationSeconds, responseMetricsQueue, coll))
        threadList.append(thread)

    # Start the threads
    for thread in threadList:
        thread.start()

    for thread in threadList:
        thread.join()

def findRandDocsForDuration(readingGroup, durationSeconds, responseMetricsQueue, coll):
    # Create simple vars from readingGroup dict
    readingGroupId = readingGroup["readingGroupId"]
    docsLower = readingGroup["docsLower"]
    docsUpper = readingGroup["docsUpper"]

    # Print process name/id and read range
    currentProcName = multiprocessing.current_process().name
    currentProcId = multiprocessing.current_process().pid
    currentThreadId = threading.current_thread().ident

    print("Starting read thread in " + currentProcName + " (PID: " + str(currentProcId) + ", TID: " + str(currentThreadId) + "). Reading docs: " + str(docsLower) + "-" + str(docsUpper))    

    # Set endtime for this process
    endtime = datetime.datetime.now() + datetime.timedelta(seconds=durationSeconds)

    
    # Loop until endtime
    while (datetime.datetime.now() < endtime):
        # Set a random doc_id within the defined range for this process
        doc_id = random.randrange(docsLower, docsUpper)

        # Time the execution of the find command
        start = timer()
        # This 'projects'away the 'blob' field to reduce network overhead, it shouldn't materially impact test results
        coll.find_one({"_id": doc_id},{"blob": 0})
        end = timer()

        # Calculate the response time in milliseconds
        responseTimeMs = ((end - start)*1000)

        # Add an entry to the responseQueue containing the id of this proess and the responseTimeMs value
        responseMetricsQueue.put({
            "readingGroupId": readingGroupId,
            "pid": currentProcId,
            "responseTimeMs": responseTimeMs
        })

def resultsHandler(responseMetricsQueue, readDurationSeconds, readingGroupsList):
    print("Results Handler Starting...")
    
    # Set start/endtimes for full test
    startTime = datetime.datetime.now()
    endTime = startTime + datetime.timedelta(seconds=(readDurationSeconds))

    # Initialise the results dictionary
    fullResultsDict = createResultsDict(readingGroupsList, startTime, endTime, readDurationSeconds)
    
    # Initialise bucket configuration
    bucketInfo = {
        "bucketNo": 0,
        "bucketDuration": datetime.timedelta(seconds=read_config.intervalResultReportingSeconds),
        "bucketStartTime": startTime,
        "bucketEndTime": startTime + datetime.timedelta(seconds=read_config.intervalResultReportingSeconds)
    }

    # Initialise the dict which 'buckets' all response times in a list
    responseTimesByReadingGroupByPidBucket = {}

    # Loop until endtime has passed and queue is empty
    while datetime.datetime.now() < (endTime) or not responseMetricsQueue.empty():
        # If we have reached the end of this buckets time and this isn't the last bucket
        if (datetime.datetime.now() > bucketInfo["bucketEndTime"]) and bucketInfo["bucketEndTime"] != endTime:
            # Add the latest results to a new bucket in each proc of each readingGroup
            fullResultsDict = addBucketOfLatestResults(responseTimesByReadingGroupByPidBucket, fullResultsDict, bucketInfo)
            # Update the summary metrics
            fullResultsDict = updateSummaryMetrics(fullResultsDict)
            # Reset the responseTimes dict
            responseTimesByReadingGroupByPidBucket = {}

            # Print interval results to stdout
            printLatestBucketResults(fullResultsDict, bucketInfo)

            # Increment the bucket count and start/end times
            bucketInfo["bucketNo"] += 1
            bucketInfo["bucketStartTime"] += bucketInfo["bucketDuration"]
            if ((bucketInfo["bucketEndTime"] + bucketInfo["bucketDuration"]) > endTime):
                bucketInfo["bucketEndTime"] = endTime
            else:
                bucketInfo["bucketEndTime"] += bucketInfo["bucketDuration"]

        # If the queue has items
        if ( not responseMetricsQueue.empty() ):
            # Get item from queue
            responseItem = responseMetricsQueue.get(True, 2)

            # Set simple vars for clarity
            pid = responseItem["pid"]
            readingGroupId = responseItem["readingGroupId"]
            responseTimeMs = responseItem["responseTimeMs"]

            # Add readingGroup to dict if it doesn't exist
            if readingGroupId not in responseTimesByReadingGroupByPidBucket:
                responseTimesByReadingGroupByPidBucket[readingGroupId] = {}
            
            # Add pid to dict if it doesn't exist
            if pid not in responseTimesByReadingGroupByPidBucket[readingGroupId]:
                responseTimesByReadingGroupByPidBucket[readingGroupId][pid] = []
            
            # Add response time to readingGroup[pid] of responseTimesByReadingGroupByPidBucket
            responseTimesByReadingGroupByPidBucket[readingGroupId][pid].append(responseTimeMs)

    # Update the results dict for the final time
    # Add the latest results to a new bucket in each proc of each readingGroup
    fullResultsDict = addBucketOfLatestResults(responseTimesByReadingGroupByPidBucket, fullResultsDict, bucketInfo)
    # Update the summary metrics
    fullResultsDict = updateSummaryMetrics(fullResultsDict)

    # Print results to stdout
    resultsTable = getResultsTable(fullResultsDict)
    print(resultsTable)

    # Write the fullResultsDict to a file
    fullResultsDictFileName = startTime.strftime("%Y%m%d-%H%M%S") + "-fullResultsDict.json"
    writeResultsToFile(json.dumps(fullResultsDict,indent=2), fullResultsDictFileName)
    
    # Write the resultsTable to a file
    resultsTableFileName = startTime.strftime("%Y%m%d-%H%M%S") + "-resultsTable.txt"
    writeResultsToFile(str(resultsTable), resultsTableFileName)

    # Print file name/location on screen
    print("Results Dict written to " + fullResultsDictFileName)
    print("Results Table written to " + resultsTableFileName)

def createResultsDict(readingGroupsList, startTime, endTime, readDurationSeconds):
    initialResultsDict = {
        "testRun": startTime.strftime("%Y%m%d%H%M%S"),
        "startTime": str(startTime),
        "endTime": str(endTime),
        "testDurationSeconds": readDurationSeconds,
        "totalDocsRetrieved": 0,
        "tps": 0,
        "averageResponseTimeMs": 0,
        "readingGroups": {}
    }
    #print(json.dumps(readingGroupsList,indent=2))
    for readingGroup in readingGroupsList:
        # Add results fields to readingGroup
        readingGroup["elapsedDurationSeconds"] = 0
        readingGroup["totalDocsRetrieved"] = 0
        readingGroup["tps"] = 0
        readingGroup["averageResponseTimeMs"] = 0
        readingGroup["noOfProcs"] = readingGroup["procs"]
        readingGroup["docsLower"] = readingGroup["docsLower"]
        readingGroup["docsUpper"] = readingGroup["docsUpper"]
        readingGroup["procs"] = {}
        
        # Add the readingGroup to the dict
        initialResultsDict["readingGroups"][readingGroup["readingGroupId"]] = readingGroup
    
    return initialResultsDict

def addBucketOfLatestResults(responseTimesByReadingGroupByPidBucket, fullResultsDict, bucketInfo):
    bucketNo = bucketInfo["bucketNo"]
    bucketStartTime = bucketInfo["bucketStartTime"]
    bucketEndTime = bucketInfo["bucketEndTime"]

    for readingGroupId in responseTimesByReadingGroupByPidBucket.keys():
        for pid in responseTimesByReadingGroupByPidBucket[readingGroupId].keys():
            docsFound = len(responseTimesByReadingGroupByPidBucket[readingGroupId][pid])
            averageResponseTimeMs = sum(responseTimesByReadingGroupByPidBucket[readingGroupId][pid]) / docsFound
            bucket = createBucket(pid, bucketNo, bucketStartTime, bucketEndTime, docsFound, averageResponseTimeMs)

            # Create the process entry (and buckets entry within) if it does not already exist
            if pid not in fullResultsDict["readingGroups"][readingGroupId]["procs"]:
                fullResultsDict["readingGroups"][readingGroupId]["procs"][pid] = {"pid": pid}
                fullResultsDict["readingGroups"][readingGroupId]["procs"][pid]["buckets"] = {}

            # Add current bucket to readingGroup/Proc in full results
            fullResultsDict["readingGroups"][readingGroupId]["procs"][pid]["buckets"][bucketNo] = bucket

    return fullResultsDict

def createBucket(pid, bucketNo, bucketStartTime, bucketEndTime, docsFound, averageResponseTimeMs):
    bucket = {
        "pid": pid,
        "bucketNo": bucketNo,
        "bucketStartTime": str(bucketStartTime),
        "bucketEndTime": str(bucketEndTime),
        "bucketDurationSecs": (bucketEndTime - bucketStartTime).seconds,
        "docsFound": docsFound,
        "tps": docsFound / (bucketEndTime - bucketStartTime).seconds,
        "averageResponseTimeMs": averageResponseTimeMs
    }

    return bucket

def updateSummaryMetrics(fullResultsDict):
    # Create a new dict for updated results, based on previous dict
    updatedResultsDict = fullResultsDict
    
    fullTestDuration = fullResultsDict["testDurationSeconds"]
    fullTestDocsRetrieved =0
    fullTestTps = 0
    fullTestTotalResponseTimeMs = 0

    # For each readingGroup in the dict
    for readingGroup in fullResultsDict["readingGroups"]:
        # initialise a series of measurement vars
        rgDocsRetrieved = 0
        rgDurationSeconds = 0
        rgTotalResponseTimeMs = 0
        rgAverageResponseTimeMs = 0
        
        # and for each process in each readingGroup
        for pid in fullResultsDict["readingGroups"][readingGroup]["procs"]:
            # initialise a series of measurement vars
            pidDocsRetrieved = 0
            pidDurationSeconds = 0
            pidTotalResponseTimeMs = 0
            pidAverageResponseTimeMs = 0

            # and then for each bucket in each process, create totals for all buckets in proc
            for bucket in fullResultsDict["readingGroups"][readingGroup]["procs"][pid]["buckets"].values():
                pidDocsRetrieved += bucket["docsFound"]
                pidDurationSeconds += bucket["bucketDurationSecs"]
                pidTotalResponseTimeMs += (bucket["docsFound"] * bucket["averageResponseTimeMs"])
                pidAverageResponseTimeMs = pidTotalResponseTimeMs / pidDocsRetrieved

            # update proc level metrics
            updatedResultsDict["readingGroups"][readingGroup]["procs"][pid]["totalDocsRetrieved"] = pidDocsRetrieved
            updatedResultsDict["readingGroups"][readingGroup]["procs"][pid]["durationSeconds"] = pidDurationSeconds
            updatedResultsDict["readingGroups"][readingGroup]["procs"][pid]["averageResponseTime"] = pidAverageResponseTimeMs
            updatedResultsDict["readingGroups"][readingGroup]["procs"][pid]["tps"] = pidDocsRetrieved / pidDurationSeconds
            
            # add proc level metrics to readingGroup level metrics
            rgDocsRetrieved += pidDocsRetrieved
            rgDurationSeconds = pidDurationSeconds
            rgTotalResponseTimeMs += pidTotalResponseTimeMs
            rgAverageResponseTimeMs = rgTotalResponseTimeMs / rgDocsRetrieved

        # update rg level metrics
        updatedResultsDict["readingGroups"][readingGroup]["totalDocsRetrieved"] = rgDocsRetrieved
        updatedResultsDict["readingGroups"][readingGroup]["elapsedDurationSeconds"] = rgDurationSeconds
        updatedResultsDict["readingGroups"][readingGroup]["averageResponseTime"] = rgAverageResponseTimeMs
        updatedResultsDict["readingGroups"][readingGroup]["tps"] = rgDocsRetrieved / rgDurationSeconds

        # Add readingGroup level metrics to fullTest metrics
        fullTestDocsRetrieved += rgDocsRetrieved
        fullTestTotalResponseTimeMs += rgTotalResponseTimeMs

    # Update full test metrics
    updatedResultsDict["totalDocsRetrieved"] = fullTestDocsRetrieved
    updatedResultsDict["tps"] = fullTestDocsRetrieved / fullTestDuration
    updatedResultsDict["averageResponseTimeMs"] = fullTestTotalResponseTimeMs / fullTestDocsRetrieved
    
    return updatedResultsDict

def printLatestBucketResults(fullResultsDict, bucketInfo):
    bucketNo = bucketInfo["bucketNo"]
    bucketStartTime = bucketInfo["bucketStartTime"]
    bucketEndTime = bucketInfo["bucketEndTime"]
    
    table = PrettyTable(['Process','Reading Group','Docs Found', 'TPS', 'Avg Response (ms)'])
    table.title = "Interval " + str(bucketNo) + " Result  (" + bucketStartTime.strftime("%H:%M:%S") + " - " + bucketEndTime.strftime("%H:%M:%S") +")"

    for readingGroup in fullResultsDict["readingGroups"]:
        for proc in fullResultsDict["readingGroups"][readingGroup]["procs"]:
            docsFound = fullResultsDict["readingGroups"][readingGroup]["procs"][proc]["buckets"][bucketNo]["docsFound"]
            averageResponseTimeMs = round(fullResultsDict["readingGroups"][readingGroup]["procs"][proc]["buckets"][bucketNo]["averageResponseTimeMs"],5)
            tps = fullResultsDict["readingGroups"][readingGroup]["procs"][proc]["buckets"][bucketNo]["tps"]
            table.add_row([proc, readingGroup, docsFound, tps, averageResponseTimeMs])

    print()
    if bucketNo == 0:
        print("WARNING: First/last interval results may not be accurate due to start time offset between resultHandler and readProcesses")
    print(table)
    print()

def getResultsTable(fullResultsDict):
    for i in range(5):
        print()

    resultsTable = PrettyTable(['TEST RESULTS'])
    resultsTable.align = 'l'
    resultsTable.add_row([getSummaryTestResultsTable(fullResultsDict)])
    resultsTable.add_row([getAllReadingGroupResultsTable(fullResultsDict)])
    resultsTable.add_row([getAllProcessResultsTable(fullResultsDict)])
    resultsTable.add_row([getAllBucketResultsTable(fullResultsDict)])
    return resultsTable

def getAllReadingGroupResultsTable(fullResultsDict):
    table = PrettyTable(['ReadingGroup','No of Procs', 'docsLower', 'docsUpper', 'Docs Found', 'TPS', 'TPS per Proc', 'Avg Response (ms)'])
    
    for readingGroup in fullResultsDict["readingGroups"]:
        docsFound = fullResultsDict["readingGroups"][readingGroup]["totalDocsRetrieved"]
        averageResponseTimeMs = round(fullResultsDict["readingGroups"][readingGroup]["averageResponseTime"],5)
        tps = fullResultsDict["readingGroups"][readingGroup]["tps"]
        procs = fullResultsDict["readingGroups"][readingGroup]["noOfProcs"]
        tpsPerProc = tps / procs
        docsLower = fullResultsDict["readingGroups"][readingGroup]["docsLower"]
        docsUpper = fullResultsDict["readingGroups"][readingGroup]["docsUpper"]

        table.add_row([readingGroup, procs, docsLower, docsUpper, docsFound, tps, tpsPerProc, averageResponseTimeMs])
    
    table.title = "All Results By Reading Group"

    return table

def getAllBucketResultsTable(fullResultsDict):
    readingGroups = fullResultsDict["readingGroups"]
    table = PrettyTable(['pid','bucketNo','bucketDuration','Docs Found', 'TPS', 'Avg Response (ms)'])
    table.title = "All Results By Process and Bucket"

    for readingGroupNo in readingGroups:
        readingGroup = readingGroups[readingGroupNo]
        for pid in readingGroup["procs"]:
            proc = readingGroup["procs"][pid]
            for bucketNo in proc["buckets"]:
                bucket = proc["buckets"][bucketNo]
                table.add_row([bucket["pid"], bucket["bucketNo"], bucket["bucketDurationSecs"], bucket["docsFound"], bucket["tps"], round(bucket["averageResponseTimeMs"],5)])
    
    return table

def getAllProcessResultsTable(fullResultsDict):
    readingGroups = fullResultsDict["readingGroups"]
    fieldsToPrint = ['pid', 'totalDocsRetrieved', 'averageResponseTime', 'tps']

    table = PrettyTable(fieldsToPrint)
    table.title = "All Results By Process"

    for readingGroupNo in readingGroups:
        readingGroup = readingGroups[readingGroupNo]
        for pid in readingGroup["procs"]:
            proc = readingGroup["procs"][pid]
            table.add_row([proc['pid'], proc['totalDocsRetrieved'], round(proc['averageResponseTime'],5), proc['tps']])
    
    return table

def getSummaryTestResultsTable(fullResultsDict):
    fieldsToPrint = ['testRun', 'testDurationSeconds', 'totalDocsRetrieved', 'tps', 'averageResponseTimeMs']
    table = PrettyTable(fieldsToPrint)
    table.title = "Summary Results"

    row = []
    for field in fieldsToPrint:
        row.append(fullResultsDict[field])
        
    table.add_row(row)
    
    return table

def writeResultsToFile(string, fileName):
    path = "./read_results/"
    if not os.path.exists(path):
        os.mkdir(path)

    fullFilePath = path + fileName

    with open(fullFilePath, 'w') as file:
        file.write(string)

def joinReadProcesses(processList):
    try:
        for process in processList:
            process.join()
    except:
        print("wuh wuh")

if __name__ == "__main__":
    main()